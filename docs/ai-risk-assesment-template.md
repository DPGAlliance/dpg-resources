# AI Risk Assessment Template

An AI risk assessment provides information on how risk is considered in developing an AI system, fostering transparency and accountability. You can use any risk assessment template when submitting a DPG application; however, to use this one, kindly make a copy of this file and answer the following questions in the **“Answers”** column to the best of your ability.

---

## Proportionality

| Questions | Answers |
|----------|---------|
| Describe the potential impact of the AI system on people, including vulnerable groups and those affected indirectly, in the context for which it was developed. (Consider how the model/system could affect these groups positively or negatively). | [Insert your answer here…] |
| Describe if and how stakeholders were engaged in the design and development of the solution. Indicate whether the system was designed based on human-centric principles, e.g. principles for digital development, FAIR principles, OECD Principles on AI, UNESCO Recommendation on the Ethics of AI, Google AI principles, etc. | [Insert your answer here…] |

> [!NOTE]
> Proportionality risks refer to the potential for AI systems to exhibit disproportionate or excessive responses, actions, or impacts relative to the intended purpose. For instance, disproportionality against a particular language, geography, population, or other characteristics that might only represent certain groups of people.

---

## Bias and Fairness

| Questions | Answers |
|----------|---------|
| What potential biases could be encoded in the AI system? | [Insert your answer here…] |
| What steps are taken to monitor, mitigate, and address biases? | [Insert your answer here…] |
| Document the results of relevant fairness assessments and further measures to monitor and reduce remaining bias. | [Insert your answer here…] |
| With which metrics have you decided to evaluate the performance and fairness of the model? Why these metrics? | [Insert your answer here…] |
| What are the thresholds used by the model (if any), and how have these been selected? | [Insert your answer here…] |
| What approach do you have in place to evaluate the model’s performance and fairness against new and/or unseen data (e.g., via cross-validation or bootstrapping)? | [Insert your answer here…] |
| How does the model handle outliers, over- and underrepresentation, and bias towards certain populations? | [Insert your answer here…] |

> [!NOTE]
> **Example biases:**
> - **Data Bias**: AI trained on biased data (e.g., historical criminal justice data) can perpetuate discriminatory practices.
> - **Algorithmic Bias**: AI systems (e.g., facial recognition) trained on unrepresentative data can lead to misidentification and harm.
> - **Outcome Bias**: Algorithms relying on socioeconomic factors can indirectly discriminate.
> - **Unintended Bias**: Training data skewed by company demographics (e.g., male-dominated) may favor similar profiles.
> - **Representation Bias**: Underrepresented dialects/languages in training data may reduce accessibility.

---

## Mitigations

| Questions | Answers |
|----------|---------|
| What steps are taken to evaluate the accuracy of the AI system outputs and make adequate decisions? | [Insert your answer here…] |
| What processes exist to implement (and document) thorough model validation and quality assurance? | [Insert your answer here…] |
| What guardrails are implemented to mitigate the consequences of erroneous outputs and to prevent future errors? | [Insert your answer here…] |
| What are the technical measures taken to ensure the robustness and security of your system against potential attacks throughout its life cycle? | [Insert your answer here…] |
| Does the AI system leave meaningful opportunities for human decision-making, oversight, and control? | [Insert your answer here…] |
| Are there measures in place to effectively appeal the model’s decision? If so, how is this process managed and communicated to the users? | [Insert your answer here…] |

> [!NOTE]
> **Example mitigation practices:**
> - Use of diverse data during training
> - Human review for high-risk scenarios
> - Appeals process with human oversight
> - Bias audits
> - Ongoing performance monitoring
> - Adversarial training for robustness

---

## Risks and Harms / Use Cases

| Questions | Answers |
|----------|---------|
| Did you perform validation experiments in the deployment context(s)? | [Insert your answer here…] |
| Could any potential harm arise from misuse or unintended use of the AI system? | [Insert your answer here…] |
| Please describe the guardrails implemented to prevent misuse and limit harmful unintended uses, and an assessment of how effective the guardrails are. | [Insert your answer here…] |

---

## Transparency

| Questions | Answers |
|----------|---------|
| What measures are taken to ensure transparency and explainability, including tools for responsible AI development or explainability? | [Insert your answer here…] |
| Does the AI system describe how the system works and/or explain the logic it uses to arrive at a particular output? | [Insert your answer here…] |
| If users interact directly with the system, have they been informed that they are interacting with an AI system, not a human? | [Insert your answer here…] |
| Is AI-generated content clearly labeled as such? | [Insert your answer here…] |

> [!NOTE]
> **Examples of transparency measures:**
> - **Explainable AI (XAI)**: Systems that provide understandable decision logic.
> - **Data Transparency**: Document source and nature of training data.
> - **Model Transparency**: Share architecture, parameters, and training details.
> - **Performance Transparency**: Report metrics like accuracy and fairness.
> - **Auditability**: Include logs/trails of decision-making.
> - **User-facing Transparency**: Inform users of AI use, data usage, and implications.

---
